{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB2aud44SoJr"
      },
      "source": [
        "<center><a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uc3Qw-nhSoJt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms.v2 as transforms\n",
        "import torchvision.io as tv_io\n",
        "\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "import utils\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoNS9vltSoJt"
      },
      "source": [
        "## The Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px7wiO0gSoJt"
      },
      "source": [
        "<img src=\"./images/fruits.png\" style=\"width: 600px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eb7GNjESoJu"
      },
      "source": [
        "## Load ImageNet Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atvM1tmaSoJu"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import vgg16\n",
        "from torchvision.models import VGG16_Weights\n",
        "\n",
        "weights = VGG16_Weights.FIXME\n",
        "vgg_model = vgg16(weights=weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uud6c-RFSoJu"
      },
      "source": [
        "## Freeze Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4ip8vrmSoJv"
      },
      "outputs": [],
      "source": [
        "# Freeze base model\n",
        "vgg_model.requires_grad_(FIXME)\n",
        "next(iter(vgg_model.parameters())).requires_grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRfwQel-SoJv"
      },
      "source": [
        "##  Add Layers to Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnTl5lJxSoJv"
      },
      "outputs": [],
      "source": [
        "vgg_model.classifier[0:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8Q1KlcuSoJv"
      },
      "source": [
        "Once we've taken what we've wanted from VGG16, we can then add our own modifications. No matter what additional modules we add, we still need to end with one value for each output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NHSBI9eSoJv"
      },
      "outputs": [],
      "source": [
        "N_CLASSES = FIXME\n",
        "\n",
        "my_model = nn.Sequential(\n",
        "    vgg_model.features,\n",
        "    vgg_model.avgpool,\n",
        "    nn.Flatten(),\n",
        "    vgg_model.classifier[0:3],\n",
        "    nn.Linear(4096, 500),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(500, N_CLASSES)\n",
        ")\n",
        "my_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Uiz4HAeSoJv"
      },
      "source": [
        "## Compile Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHzukerPSoJw"
      },
      "outputs": [],
      "source": [
        "loss_function = nn.FIXME()\n",
        "optimizer = Adam(my_model.parameters())\n",
        "my_model = torch.compile(my_model.to(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dYSKKeuSoJw"
      },
      "source": [
        "##  Data Transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl6Fhc6ISoJw"
      },
      "source": [
        "To preprocess our input images, we will use the transforms included with the VGG16 weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85KWNshcSoJw"
      },
      "outputs": [],
      "source": [
        "pre_trans = weights.transforms()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pg9aaOh3SoJw"
      },
      "outputs": [],
      "source": [
        "IMG_WIDTH, IMG_HEIGHT = (224, 224)\n",
        "\n",
        "random_trans = transforms.Compose([\n",
        "    FIXME\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFe9GIb7SoJw"
      },
      "source": [
        "##  Load Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7V5HHFOSoJw"
      },
      "source": [
        "Now it's time to load the train and validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "St4a-z60SoJw"
      },
      "outputs": [],
      "source": [
        "DATA_LABELS = [\"freshapples\", \"freshbanana\", \"freshoranges\", \"rottenapples\", \"rottenbanana\", \"rottenoranges\"]\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data_dir):\n",
        "        self.imgs = []\n",
        "        self.labels = []\n",
        "\n",
        "        for l_idx, label in enumerate(DATA_LABELS):\n",
        "            data_paths = glob.glob(data_dir + label + '/*.png', recursive=True)\n",
        "            for path in data_paths:\n",
        "                img = tv_io.read_image(path, tv_io.ImageReadMode.RGB)\n",
        "                self.imgs.append(pre_trans(img).to(device))\n",
        "                self.labels.append(torch.tensor(l_idx).to(device))\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.imgs[idx]\n",
        "        label = self.labels[idx]\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97XWFrwTSoJx"
      },
      "source": [
        "Select the batch size `n` and set `shuffle` either to `True` or `False` depending on if we are `train`ing or `valid`ating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hrpcLRRSoJx"
      },
      "outputs": [],
      "source": [
        "n = FIXME\n",
        "\n",
        "train_path = \"data/fruits/train/\"\n",
        "train_data = MyDataset(train_path)\n",
        "train_loader = DataLoader(train_data, batch_size=n, shuffle=FIXME)\n",
        "train_N = len(train_loader.dataset)\n",
        "\n",
        "valid_path = \"data/fruits/valid/\"\n",
        "valid_data = MyDataset(valid_path)\n",
        "valid_loader = DataLoader(valid_data, batch_size=n, shuffle=FIXME)\n",
        "valid_N = len(valid_loader.dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN2XsiKfSoJx"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlEoMYxWSoJx"
      },
      "source": [
        "Time to train the model! We've moved the `train` and `validate` functions to our [utils.py](./utils.py) file. Before running the below, make sure all your variables are correctly defined.\n",
        "\n",
        "It may help to rerun this cell or change the number of `epochs`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UOz4PeBSoJx"
      },
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print('Epoch: {}'.format(epoch))\n",
        "    utils.train(my_model, train_loader, train_N, random_trans, optimizer, loss_function)\n",
        "    utils.validate(my_model, valid_loader, valid_N, loss_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv92i7RISoJx"
      },
      "source": [
        "## Unfreeze Model for Fine Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO1UbX7NSoJx"
      },
      "source": [
        "If you have reached 92% validation accuracy already, this next step is optional. If not, we suggest fine tuning the model with a very low learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pMbDmiiSoJx"
      },
      "outputs": [],
      "source": [
        "# Unfreeze the base model\n",
        "vgg_model.requires_grad_(FIXME)\n",
        "optimizer = Adam(my_model.parameters(), lr=.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uwSNBWVSoJx"
      },
      "outputs": [],
      "source": [
        "epochs = 1\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print('Epoch: {}'.format(epoch))\n",
        "    utils.train(my_model, train_loader, train_N, random_trans, optimizer, loss_function)\n",
        "    utils.validate(my_model, valid_loader, valid_N, loss_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjcVEkOoSoJy"
      },
      "source": [
        "##  Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugRK3JnYSoJ6"
      },
      "outputs": [],
      "source": [
        "utils.validate(my_model, valid_loader, valid_N, loss_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqZJVj1qSoJ7"
      },
      "source": [
        "<img src=\"./images/assess_task.png\" style=\"width: 800px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNLxIZWJSoJ7"
      },
      "source": [
        "<center><a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a></center>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}